{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/CompVis/stable-diffusion-v1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'CompVis/stable-diffusion-v1-4'\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "prompt = 'a photo of an astronaut riding a horse on mars'\n",
    "prompt = 'white image'\n",
    "\n",
    "path = '../images/CompVis/sd_v14/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the pipeline with the default PNDM scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "# device = \"cuda\"\n",
    "\n",
    "\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(device)\n",
    "\n",
    "# prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "# image = pipe(prompt).images[0]  \n",
    "    \n",
    "# image.save(\"astronaut_rides_horse.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 10.11it/s]\n",
      "100%|██████████| 50/50 [00:46<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    "    safety_checker = None,\n",
    "    requires_safety_checker = False,\n",
    ")\n",
    "\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "prompt = 'anime girl with headset gamer'\n",
    "\n",
    "image = pipe(prompt).images[0]\n",
    "    \n",
    "image.save(path+'2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you are limited by GPU memory and have less than 4GB of GPU RAM available, please make sure to load the StableDiffusionPipeline in float16 precision instead of the default float32 precision as done above.\n",
    "You can do so by telling diffusers to expect the weights to be in float16 precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)\n",
    "pipe.enable_attention_slicing()\n",
    "\n",
    "image = pipe(prompt).images[0]  \n",
    "    \n",
    "image.save(path+'2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To swap out the noise scheduler, pass it to from_pretrained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Euler scheduler here instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "image = pipe(prompt).images[0]  \n",
    "    \n",
    "image.save(path+'3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX/Flax\n",
    "To use StableDiffusion on TPUs and GPUs for faster inference you can leverage JAX/Flax.\n",
    "\n",
    "Running the pipeline with default PNDMScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "from flax.jax_utils import replicate\n",
    "from flax.training.common_utils import shard\n",
    "\n",
    "from diffusers import FlaxStableDiffusionPipeline\n",
    "\n",
    "pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    revision=\"flax\",\n",
    "    dtype=jax.numpy.bfloat16\n",
    ")\n",
    "\n",
    "prng_seed = jax.random.PRNGKey(0)\n",
    "num_inference_steps = 50\n",
    "\n",
    "num_samples = jax.device_count()\n",
    "prompt = num_samples * [prompt]\n",
    "prompt_ids = pipeline.prepare_inputs(prompt)\n",
    "\n",
    "# shard inputs and rng\n",
    "params = replicate(params)\n",
    "prng_seed = jax.random.split(prng_seed, num_samples)\n",
    "prompt_ids = shard(prompt_ids)\n",
    "\n",
    "images = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\n",
    "images = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you are limited by TPU memory, please make sure to load the FlaxStableDiffusionPipeline in bfloat16 precision instead of the default float32 precision as done above. You can do so by telling diffusers to load the weights from \"bf16\" branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "from flax.jax_utils import replicate\n",
    "from flax.training.common_utils import shard\n",
    "\n",
    "from diffusers import FlaxStableDiffusionPipeline\n",
    "\n",
    "pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    revision=\"bf16\",\n",
    "    dtype=jax.numpy.bfloat16\n",
    ")\n",
    "\n",
    "prng_seed = jax.random.PRNGKey(0)\n",
    "num_inference_steps = 50\n",
    "\n",
    "num_samples = jax.device_count()\n",
    "prompt = num_samples * [prompt]\n",
    "prompt_ids = pipeline.prepare_inputs(prompt)\n",
    "\n",
    "# shard inputs and rng\n",
    "params = replicate(params)\n",
    "prng_seed = jax.random.split(prng_seed, num_samples)\n",
    "prompt_ids = shard(prompt_ids)\n",
    "\n",
    "images = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\n",
    "images = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
